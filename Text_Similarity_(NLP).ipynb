{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPREZOw468kHdqBKUd916Jl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaustubhhatkar/DataScience_and_MachineLearning/blob/master/Text_Similarity_(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-mnzxA0M703",
        "outputId": "539a94aa-70f7-454e-fe76-1e053fac3012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Necessary libraries"
      ],
      "metadata": {
        "id": "h06Q8qjNNzBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import requests\n",
        "import random\n",
        "import pickle\n",
        "\n"
      ],
      "metadata": {
        "id": "YfDOL6HTN5tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "jd=\"\"\"Machine Learning Engineering 13585ABC Knowledge and Innovation What youâ€™ll do You wil focus on researching, building, and designing self-running artificial inteligence (AI) systems to automate predictive models. You are responsible to design and create the AI algorithms capable of learning and making predictions that define machine learning (ML). Experience and qualifications Bachelor's or Master's degree (mention the course as per requirement) 0-2 years of work experience providing analytics solutions in a commercial setting Technical expertise Must have Machine Learning, Clustering, Logistic Regression, Classification Different libraries such as SciKit Learn, NumPy, Pandas, Matplotlib, Seaborn Deep learning frameworks such as Tensorflow, Keras, PyTorch and application of Neural Networks and models. CNN, RNN, GANs Familiar with Natural Language Processing and associated libraries like NLTK, SpaCy, Beautiful Soup PySpark, Hadoop, and Big Data Pipelines Data science methodology from exploratory data analysis, feature engineering, model selection, deployment of the model at scale, and model evaluation Deploying NLP architectures and Computer Vision Models in production Considered as a plus Transformers and other advance techniques in NLP Familiar with Computer Vision models and object detection, OCR, OpenCV Analytical Tools as it wil reduce any medium for data transfers Web frameworks like Django and databases like MongoDB, NoSQL, GraphQL SQL, Firebase, AWS, Azure, Google Cloud Platform Your job type Ful time.\"\"\"\n",
        "\n",
        "resume=\"\"\"RACHEL DAWN - DATA CIENTIST PERSONAL PROFILE WORK BACKGROUND The majority of my work experience Innoplexus includes Computer Vision, Deep Learning Data Scientist Nov 2019 to Till Date and NLP. I have worked on traditional machine learning algorithms like SVM, Random Forest, Logistic Regression, Linear Build an end-to-end, Computer Vision and NLP-based PDF-Extractor Regression, KNN, K-means Clustering, etc. model to convert unstructured congress research paper pdf documents and Deep Learning algorithms like YOLO, into a structured and categorical format with a micro-average F1-score Baidu's DeepSpeech, FCN on Keras and of 0.84. Pytorch architectures. Created a model for identification and conversion of the graphical representations of chemical structures from literature sources into SMILES. Used OpenCV for various image preprocessing and OSRA an open-sourced tool for SMILES generation. SKILLS EDUCATION Deep learning, Python, SQL , Keras, Dual Degree (B.tech + M.Tech)(Chemical from Indian Institute of PyTorch, PySpark , Data Science, Data Technology (IIT), Kharagpur in 2019 Analysis, Machine Learning, Natural Language Processing, Artificial Intelligence, Computer Vision, Pattern Recognition, NLP, OpenCV, Image Processing. PROJECTS OTHER ACTIVITIES End-to-End Computer Vision Model for Natural Language Mastery - DataCamp Hindi Sentence Recognition.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0IOMY8EZVD-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "6up6AEVR2v2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Cleaning(text):\n",
        "  def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "  def replaceUrls(data):\n",
        "      #Removing URLs with a regular expression\n",
        "      url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "      data = url_pattern.sub(r'', data)\n",
        "      return data  \n",
        "  def removeEmail(data):\n",
        "      # Remove Emails\n",
        "      data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "      return data\n",
        " \n",
        "\n",
        "  def misc(data):\n",
        "      # Remove new line characters\n",
        "      data = re.sub(r'\\.+', \".\", data)\n",
        "      data = re.sub('\\s+', ' ', data)\n",
        "      # Remove distracting single quotes\n",
        "      data = re.sub(\"\\'\", \"\", data)\n",
        "      return data\n",
        "\n",
        "  sentence = cleanhtml(text)\n",
        "  sentence = replaceUrls(sentence)\n",
        "  sentence = removeEmail(sentence)\n",
        "  sentence = misc(sentence)\n",
        "  sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
        "  sentence = re.sub(' +', ' ', sentence)\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  return sentence "
      ],
      "metadata": {
        "id": "KOvNJG2PXYwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK2IXAk-yvqI",
        "outputId": "2aa9737b-bbc1-4700-a814-5543a864d7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_list=stopwords.words('english')\n",
        "def removeStopwords(sentence):\n",
        "  words=sentence.split(\" \")\n",
        "  filtered_sentence=[word for word in words if not word in stopwords_list]\n",
        "  ans=' '.join([i for i in filtered_sentence if len(i)>=2])\n",
        "\n",
        "  return ans"
      ],
      "metadata": {
        "id": "taDtEuC9y0nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_final=Cleaning(jd)\n",
        "print(pre_final)\n",
        "pre_final=removeStopwords(pre_final)\n",
        "print(pre_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDOtOz4B20xb",
        "outputId": "6143c3e0-7fb5-48f4-98e3-380f7c88847d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machine learning engineering abc knowledge and innovation what you ll do you wil focus on researching building and designing self running artificial inteligence ai systems to automate predictive models you are responsible to design and create the ai algorithms capable of learning and making predictions that define machine learning ml experience and qualifications bachelors or masters degree mention the course as per requirement years of work experience providing analytics solutions in a commercial setting technical expertise must have machine learning clustering logistic regression classification different libraries such as scikit learn numpy pandas matplotlib seaborn deep learning frameworks such as tensorflow keras pytorch and application of neural networks and models cnn rnn gans familiar with natural language processing and associated libraries like nltk spacy beautiful soup pyspark hadoop and big data pipelines data science methodology from exploratory data analysis feature engineering model selection deployment of the model at scale and model evaluation deploying nlp architectures and computer vision models in production considered as a plus transformers and other advance techniques in nlp familiar with computer vision models and object detection ocr opencv analytical tools as it wil reduce any medium for data transfers web frameworks like django and databases like mongodb nosql graphql sql firebase aws azure google cloud platform your job type ful time \n",
            "machine learning engineering abc knowledge innovation wil focus researching building designing self running artificial inteligence ai systems automate predictive models responsible design create ai algorithms capable learning making predictions define machine learning ml experience qualifications bachelors masters degree mention course per requirement years work experience providing analytics solutions commercial setting technical expertise must machine learning clustering logistic regression classification different libraries scikit learn numpy pandas matplotlib seaborn deep learning frameworks tensorflow keras pytorch application neural networks models cnn rnn gans familiar natural language processing associated libraries like nltk spacy beautiful soup pyspark hadoop big data pipelines data science methodology exploratory data analysis feature engineering model selection deployment model scale model evaluation deploying nlp architectures computer vision models production considered plus transformers advance techniques nlp familiar computer vision models object detection ocr opencv analytical tools wil reduce medium data transfers web frameworks like django databases like mongodb nosql graphql sql firebase aws azure google cloud platform job type ful time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_final2=Cleaning(resume)\n",
        "print(pre_final2)\n",
        "pre_final2=removeStopwords(pre_final2)\n",
        "print(pre_final2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqmRur6I-JUo",
        "outputId": "3576db9f-5160-4d60-d178-01ead32d8821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rachel dawn data cientist personal profile work background the majority of my work experience innoplexus includes computer vision deep learning data scientist nov to till date and nlp i have worked on traditional machine learning algorithms like svm random forest logistic regression linear build an end to end computer vision and nlp based pdf extractor regression knn k means clustering etc model to convert unstructured congress research paper pdf documents and deep learning algorithms like yolo into a structured and categorical format with a micro average f score baidus deepspeech fcn on keras and of pytorch architectures created a model for identification and conversion of the graphical representations of chemical structures from literature sources into smiles used opencv for various image preprocessing and osra an open sourced tool for smiles generation skills education deep learning python sql keras dual degree b tech m tech chemical from indian institute of pytorch pyspark data science data technology iit kharagpur in analysis machine learning natural language processing artificial intelligence computer vision pattern recognition nlp opencv image processing projects other activities end to end computer vision model for natural language mastery datacamp hindi sentence recognition \n",
            "rachel dawn data cientist personal profile work background majority work experience innoplexus includes computer vision deep learning data scientist nov till date nlp worked traditional machine learning algorithms like svm random forest logistic regression linear build end end computer vision nlp based pdf extractor regression knn means clustering etc model convert unstructured congress research paper pdf documents deep learning algorithms like yolo structured categorical format micro average score baidus deepspeech fcn keras pytorch architectures created model identification conversion graphical representations chemical structures literature sources smiles used opencv various image preprocessing osra open sourced tool smiles generation skills education deep learning python sql keras dual degree tech tech chemical indian institute pytorch pyspark data science data technology iit kharagpur analysis machine learning natural language processing artificial intelligence computer vision pattern recognition nlp opencv image processing projects activities end end computer vision model natural language mastery datacamp hindi sentence recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#this file has the information on every common word and its embedding\n",
        "save_data=open(r\"drive/MyDrive/Dataset/Models/word_embeddings_smaller.pickle\", \"rb\")\n",
        "word_embed = pickle.load(save_data)\n",
        "save_data.close()\n"
      ],
      "metadata": {
        "id": "KapX7lGF-kJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embed['data']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSYY4iO-psI",
        "outputId": "29c72ad2-ca5c-4e00-de35-bb6415f7a290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.36239132, -0.6376576 , -0.61950606, -0.4612036 , -0.61091864,\n",
              "       -0.8652799 ,  0.23148443,  0.46113586, -0.5749913 ,  0.33897504,\n",
              "       -1.2678416 ,  0.30391195, -0.5054661 ,  1.6345744 ,  0.14852168,\n",
              "       -0.18458976, -0.01300318,  0.19791895, -0.7137795 , -0.02685414,\n",
              "        0.86899185,  0.21645363,  0.8548136 ,  0.12171498, -0.45416778,\n",
              "       -0.01755559,  0.39791077,  0.25650325, -0.26046163, -1.022035  ,\n",
              "        0.09318838,  0.22358201, -0.9487669 , -0.6284835 ,  0.43316552,\n",
              "       -0.8323823 , -0.7597048 ,  0.06611109,  0.42232895,  0.27570227,\n",
              "        0.17054436,  1.5929366 , -0.16462785, -0.78299934, -0.5730217 ,\n",
              "        0.31976703,  0.34705192,  0.7574879 , -0.70682687, -0.0505382 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating text vectors\n",
        "import numpy as np\n",
        "count=0\n",
        "document_embeddings=np.zeros(50,dtype=float)\n",
        "try:\n",
        "  for word in pre_final.split(\" \"):\n",
        "    array=np.asarray(word_embed[word],dtype=float)\n",
        "    document_embeddings=np.add(document_embeddings,array,out=document_embeddings,casting=\"unsafe\")\n",
        "except:\n",
        "  count=count+1\n",
        "\n",
        "print(\"Number of Words in text 1: \",len(pre_final.split(\" \")))\n",
        "print(\"Unrecognizable word count in text 1: \",count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNsAbUZQEW2C",
        "outputId": "5ca63017-e31d-4efb-83fe-27bcc998d6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Words in text 1:  159\n",
            "Unrecognizable word count in text 1:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count2 = 0\n",
        "document_embeddings2 = np.zeros(50, dtype=float)\n",
        "try:\n",
        "  for word in pre_final2.split(\" \"):\n",
        "    array = np.asarray(word_embed[word], dtype=float)\n",
        "    document_embeddings2 = np.add(document_embeddings2, array, out=document_embeddings2, casting=\"unsafe\")\n",
        "except:\n",
        "  count2 = count2 + 1\n",
        "\n",
        "print(\"Number of words in text 1:\", len(pre_final2.split(\" \")))\n",
        "print(\"Unrecognizable word count in text 1:\", count2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCRnx9AzITLI",
        "outputId": "fc62c32d-de3d-4727-e10a-5c90d8396278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in text 1: 148\n",
            "Unrecognizable word count in text 1: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSrK2dAvItsI",
        "outputId": "6d7ac3fa-55ae-4aa6-d2c4-fae533ce9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.54136997, -1.00169811, -0.99810374, -0.65893228, -0.8737345 ,\n",
              "       -1.3198083 ,  0.38000366,  0.77959137, -0.92080894,  0.61065055,\n",
              "       -1.89896479,  0.61922409, -0.86004563,  2.5504694 ,  0.20681175,\n",
              "       -0.28327482, -0.08554658,  0.26114271, -1.16649702,  0.05204532,\n",
              "        1.26061946,  0.34942682,  1.36074778,  0.16731976, -0.72149244,\n",
              "        0.05481694,  0.62491424,  0.42448563, -0.39275278, -1.54324707,\n",
              "        0.09399691,  0.36292967, -1.4869768 , -0.99597728,  0.77488923,\n",
              "       -1.28500655, -1.24286994,  0.03353305,  0.64449547,  0.4265385 ,\n",
              "        0.24397457,  2.56594741, -0.19558442, -1.15162811, -0.90070888,\n",
              "        0.56072012,  0.56175804,  1.1576283 , -1.17220959, -0.12070583])"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_embeddings2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gbkcoP7I_Fe",
        "outputId": "0b5251e0-970b-4581-9890-123cbe6f1e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.6226516 , -1.00859042, -0.98305207, -0.67888809, -0.95313039,\n",
              "       -1.40948449,  0.33849983,  0.70884381, -0.86183663,  0.49499981,\n",
              "       -2.02491487,  0.48545689, -0.75303922,  2.58070414,  0.22405648,\n",
              "       -0.31570611,  0.00726098,  0.3355594 , -1.18207653, -0.05379775,\n",
              "        1.43520461,  0.2870142 ,  1.36169974,  0.18209616, -0.72338677,\n",
              "       -0.05991598,  0.67269285,  0.44605253, -0.42309005, -1.6552187 ,\n",
              "        0.12827903,  0.33270961, -1.50367688, -0.98532811,  0.62007063,\n",
              "       -1.28920609, -1.15168964,  0.06374794,  0.63373963,  0.43418423,\n",
              "        0.27549926,  2.39833464, -0.30010049, -1.27471507, -0.82676723,\n",
              "        0.52453139,  0.49677033,  1.19385952, -1.10959661, -0.05330731])"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(A, B):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        A: a numpy array which corresponds to a word vector\n",
        "        B: A numpy array which corresponds to a word vector\n",
        "    Output:\n",
        "        cos: numerical number representing the cosine similarity between A and B.\n",
        "    \"\"\"\n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.sqrt(np.dot(A, A))\n",
        "    normb = np.sqrt(np.dot(B, B))\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos"
      ],
      "metadata": {
        "id": "4ZBgB5CWJZhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = cosine_similarity(document_embeddings, document_embeddings2)\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyXUOILYJme9",
        "outputId": "7ee012f9-6b4f-4530-f3dd-99d76799a0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9968946461253456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Sklearn Metrics"
      ],
      "metadata": {
        "id": "qQRN3dR1f5ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2 \n",
        "    \n",
        "# creating a pdf file object \n",
        "pdfFileObj = open('candidate_001.pdf', 'rb') \n",
        "    \n",
        "# creating a pdf reader object \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "    \n",
        "# printing number of pages in pdf file \n",
        "print(pdfReader.numPages) \n",
        "    \n",
        "# creating a page object \n",
        "pageObj = pdfReader.getPage(0) \n",
        "    \n",
        "# extracting text from page \n",
        "print(pageObj.extractText()) \n",
        "    \n",
        "# closing the pdf file object \n",
        "pdfFileObj.close() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "-6qOOEUeomGI",
        "outputId": "082099d5-6f06-441a-c3f2-89fd9855f41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-316-57ac0632bfe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# creating a pdf file object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpdfFileObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'candidate_001.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "7BBVZJ9-pg8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=[resume,jd]"
      ],
      "metadata": {
        "id": "Gnbs9kuGf9_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "count_matrix=cv.fit_transform(text)"
      ],
      "metadata": {
        "id": "HVf2T25Lnb8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(count_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKwvOlVqnwYm",
        "outputId": "80d78cb0-f47f-4491-e346-68eba22f7a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.56659459]\n",
            " [0.56659459 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matchPercentage=cosine_similarity(count_matrix)[0][1]*100\n",
        "print(matchPercentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSHMCdDjoLbG",
        "outputId": "058657d5-edc2-45b8-f38e-b901fad32f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.65945864684819\n"
          ]
        }
      ]
    }
  ]
}